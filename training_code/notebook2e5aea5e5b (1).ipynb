{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":67357,"databundleVersionId":8951125,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from arc_loader import *\n#print(\"Available keys:\", list(arc_test_set.queries.keys()))\nfirst_key = list(arc_test_set.queries.keys())[0]\nformatter = ArcFormatter_premix_3()\nexample = arc_test_set.get('18419cfa', formatter)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clone the tied embeddings so that lm_head gets its own copy\nmodel.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n\n# Save the untied model\nuntied_model_dir = \"kaggle/working/model\"\nmodel.save_pretrained(untied_model_dir)\nmodel.config.save_pretrained(untied_model_dir)\n\n# Later, load the model from the untied checkpoint for further use\nmodel = AutoModelForCausalLM.from_pretrained(untied_model_dir)\n\n\nmerged_model = model.merge_and_unload()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel, PeftConfig","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    if \"lora\" in name:\n        print(name, param.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the LoRA configuration\nconfig = PeftConfig.from_pretrained(\"jakebentley2001/arc-models\")\n\n# Load the base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\n# Load the LoRA adapter\nmodel = PeftModel.from_pretrained(model, \"jakebentley2001/arc-models\")\n\nmerged_model = model.merge_and_unload()\n# # Generate text\n# inputs = tokenizer(\"Your prompt here\", return_tensors=\"pt\").to(model.device)\n# outputs = model.generate(**inputs, max_length=100)\n# print(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(example)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile arc_loader.py\nimport json\nimport numpy as np\nimport hashlib\nimport os, sys\nfrom tqdm import tqdm\nfrom glob import glob\nimport itertools\nimport random\n\ndef cut_at_token(output, token_id):\n    eos_positions = (output==token_id).nonzero()[0]\n    return output[:eos_positions[0]] if len(eos_positions) else output\n\n\ndef permute_mod(a, descriptor, invert = False):\n    # Extract numbers from descriptor (e.g., \"permute0123\" -> [0,1,2,3])\n    permutation = [int(i) for i in descriptor if str(i).isdigit()]\n    # Verify permutation is valid (contains all digits 0-9)\n    assert sorted(permutation)==list(range(10))\n    a = np.asarray(a)\n    assert a.ndim==2\n    if invert: permutation = np.argsort(permutation)\n    a = np.asarray(permutation)[a]\n    return a\n\nclass ArcDataset(object):\n    # We use statis method\n    # Can be called without creating an instance\n    #transformed_array = ArcDataset.forward_mod(array, \"rot90.transpose\")\n    @staticmethod\n    def forward_mod(a, key, use_perm=True, is_output=True):\n        if a is None: return a\n        # if key = \"input.rot90.transpose\": split('.') gives [\"input\", \"rot90\", \"transpose\"]\n        #[1:] gives [\"rot90\", \"transpose\"]\n        for op in key.split('.')[1:]:\n            if op.startswith('I'):\n                if is_output: continue\n                op = op[1:]\n            if op=='rot90':                a = np.rot90(a)\n            elif op=='transpose':          a = np.swapaxes(a, 0, 1)\n            elif op.startswith('permute'): a = permute_mod(a, op, invert = False) if use_perm else a\n            elif op.startswith('copy'):    a = np.copy(a)\n            elif op.startswith('out'):     a = a\n            elif op.startswith('ex'):      a = a\n            elif op.startswith('fix'):     a = a\n            else: raise NotImplementedError(f\"Inversion of operation '{op}' unknown.\")\n        return a\n\n    @staticmethod\n    # Does the same thing but backwards\n    def invert_mod(a, key, inv_perm=True, is_output=True):\n        if a is None: return a\n        for op in key.split('.')[1:][::-1]:\n            if op.startswith('I'):\n                if is_output: continue\n                op = op[1:]\n            if   op=='rot90':              a = np.rot90(np.rot90(np.rot90(a)))\n            elif op=='transpose':          a = np.swapaxes(a, 0, 1)\n            elif op.startswith('permute'): a = permute_mod(a, op, invert=True) if inv_perm else a\n            elif op.startswith('copy'):    a = np.copy(a)\n            elif op.startswith('out'):     a = a\n            elif op.startswith('ex'):      a = a\n            elif op.startswith('fix'):     a = a\n            elif op.startswith('ice'):     a = a  # for adding icecuber solutions\n            else: raise NotImplementedError(f\"Inversion of operation '{op}' unknown.\")\n        return a\n\n    def __init__(self, queries, replies={}, keys = None, is_orig=False, is_fake=False):\n        if keys is not None: keys = [k for k in keys if k is not None]\n        self.queries = queries if keys is None else {k: queries[k] for k in keys}\n        self.replies = replies if keys is None else {k: replies[k] for k in keys if k in replies}\n        self.is_orig = is_orig\n        self.is_fake = is_fake\n        self.keys = sorted(queries.keys()) if keys is None else keys\n        self.faulty = {}\n        self.transposed_dataset = None\n        \n    # A normal instance method requires an instance to exist first\n    # But this method's purpose is to CREATE an instance\n    @classmethod\n    def empty(cls):\n        return cls(queries={}, replies={}, keys=[])\n\n    #Creates a new instance using the same class (self.__class__)\n    #Preserves the original queries and replies Uses the new keys provided\n    def change_keys(self, keys, keep_flags=False):\n        flags = dict(is_fake=self.is_fake, is_orig=self.is_orig) if keep_flags else {}\n        return self.__class__(queries = self.queries, replies = self.replies, keys = keys, **flags)\n\n    @classmethod\n    def from_file(cls, queries_file):\n        print(f\"*** Load Challenges from '{queries_file}'...\")\n        with open(queries_file) as f: queries = f.read()\n        is_fake = hashlib.md5(queries.encode('utf-8')).hexdigest().lower()=='a6b7dac3cab03abf2eb333e16610d6dc'\n        if is_fake: print(\"*** Fake test detected\")\n        return cls(\n            queries=json.loads(queries),\n            is_fake=is_fake,\n            is_orig=True,\n        )\n\n    def get(self, key, formatter):\n        assert formatter.out2_token is None or key in self.replies\n        # Takes the training examples for this key and formats them using the formatter\n        train = formatter.fmt_train(self.queries[key]['train'])\n        # Takes the test example and formats it as a query\n        query = formatter.fmt_query(self.queries[key]['test'], i=len(self.queries[key]['train']))\n        # If key exists in replies: - Format the reply using formatter\n        # also Include faulty information if it exists\n        reply = formatter.fmt_reply(self.replies[key], self.faulty.get(key)) if key in self.replies else ''\n        # adds everything together\n        text = train + query + reply if reply else formatter.fmt_train(self.queries[key]['train'], last_is_challenge=True)\n        return dict(key=key,\n                   train=train,\n                   query=query,\n                   reply=reply,\n                   input=train+query,\n                   text=text)\n\n\n    ### CLASS IS NOT DONE BUT I THINK THERE IS ENOUGH TO DISPLAY\n\ndef get_class_MyDataCollator(cache=[]):\n    if not cache:\n        from trl import DataCollatorForCompletionOnlyLM\n        class MyDataCollator(DataCollatorForCompletionOnlyLM):\n            def setup(self, out2_token_id=None, fault_token_id = None, fault_freq = 0, sample_tries = 8, mask_first_output=False):\n                self.out2_token_id = out2_token_id\n                self.fault_token_id = fault_token_id\n                self.fault_freq = fault_freq\n                self.sample_tries = sample_tries\n                self.mask_first_output = mask_first_output\n                return self\n\n            def torch_call(self, examples):\n                # Call the parent class's torch_call method to get the initial batch, This typically handles basic tokenization and padding\n                batch = super().torch_call(examples)\n                if self.out2_token_id is not None:\n                    # Ensure fault_freq is not being used when out2_token_id is active (These are mutually exclusive features\n                    assert not self.fault_freq\n                    # Process each example in the batch\n                    # Original sequence (simplified):\n                    # original = ['I',  input prefix '1', '2', '3', input grid\n                    # '5',  out2_token '7', '8', '9',  output grid 'O'  output prefix ]\n                    # After transformation:\n                    #transformed = ['I', input prefix, '7', '8', '9', input grid (copied from output),\n                    # '5',  out2_token, '7', '8', '9',  output grid, 'O'  output prefix]\n                    for i in range(len(batch['input_ids'])):\n                        # Find the last non-masked position in the labels -100 is the standard mask token in PyTorch transformers\n                        end_pos = ((batch['labels'][i] != -100).nonzero().max()).item() + 1\n                        # Find the position of the last out2_token in the sequence\n                        mid_pos = ((batch['labels'][i] == self.out2_token_id).nonzero().max()).item() + 1\n                        # Calculate the beginning position to create a symmetric masking pattern # This creates a window of equal size before and after the out2_token\n                        beg_pos = mid_pos - (end_pos - mid_pos)\n                        # Copy the labels from after the out2_token to before it # This creates a symmetric pattern around the out2_token\n                        batch['labels'][i][beg_pos:mid_pos] = batch['labels'][i][mid_pos:end_pos]\n                elif self.fault_freq:\n                    for i in range(len(batch['input_ids'])):\n                        end_pos = ((batch['labels'][i] != -100).nonzero().max()).item() + 1\n                        if not isinstance(self.fault_freq, float):\n                            eos_token_id = batch['labels'][i][end_pos - 1]\n                            # Count how many times this EOS token appears in the sequence (minus 1 for the last one)\n                            num_examples = (batch['labels'][i] == eos_token_id).sum().item() - 1\n                            # Get the fault frequency value based on the number of examples\n                            fault_freq = self.fault_freq[num_examples]\n                        else: fault_freq = self.fault_freq\n                        if random.random() < fault_freq:\n                            # Find the beginning position of the actual content (after any padding) # This is the first position after the last -100 (padding) token\n                            beg_pos = ((batch['labels'][i][:end_pos]==-100).nonzero().max()).item() + 1\n                            # Randomly select a position to introduce the fault # We use end_pos-2 to ensure we leave at least one token after the fault\n                            fault_pos = random.randint(beg_pos, end_pos-2)\n                            # Get the token at the fault position to use as reference\n                            fault_tok = batch['labels'][i][fault_pos].item()\n                            # Try to find a different token to use as the fault\n                            # We make multiple attempts (sample_tries) to find a suitable replacement\n                            for t in range(self.sample_tries):\n                                new_tok = batch['labels'][i][random.randint(beg_pos, end_pos-2)].item()\n                                # If we found a different token than the original, use it\n                                if fault_tok!=new_tok:\n                                    batch['input_ids'][i][fault_pos] = new_tok\n                                    # Mark all tokens after the fault position as fault tokens # This helps the model learn to handle and recover from faults\n                                    batch['labels'][i][fault_pos+1:end_pos] = self.fault_token_id\n                                    break\n                    \n                for i in range(len(batch['labels'])):\n                    for _ in range(self.mask_first_output):\n                        beg_pos = ((batch['labels'][i] != -100).nonzero().min()).item()\n                        mid_pos = ((batch['labels'][i][beg_pos:] == -100).nonzero().min()).item() + beg_pos\n                        end_pos = ((batch['labels'][i] != -100).nonzero().max()).item() + 1\n                        if mid_pos < end_pos: batch['labels'][i][beg_pos:mid_pos] = -100\n                return batch\n            cache.append(MyDataCollator)\n        return cache[0]\n        # https://chatgpt.com/c/67f7e5cb-1664-8011-8fbb-30615ca06601\n                        \n                        \n                        # if not isinstance(self.fault_freq, float):\n                        #     eos_token_id = batch['labels'][i][end_pos - 1]\n\n    \n    \n    \n\nclass ArcFormatter(object):\n    def __init__(self, inp_prefix, out_prefix, arr_sep, \\\n                     out2_use=False, out2_token=None, arr_beg='', \\\n                     arr_end='', pretext='', pre_out=None, exa_sep='', \\\n                     exa_end='', qry_prefix=None, rpl_prefix=None, rpl_sep=None, \\\n                     dec_sep=None, min_wid=0, min_pad='', pretext_corpus_split='', \\\n                     masking=0, tokenizer=None, collator_kwargs={}, repeat_input_aug=None, \\\n                     repeat_input_pre=None):\n            self.tokenizer = tokenizer\n            self.inp_prefix = inp_prefix\n            self.out_prefix = out_prefix\n            self.out2_token = out2_token\n            self.out2_use = out2_use\n            assert not out2_use or out2_token is not None\n            assert not out2_use or masking in [1,2]\n            assert masking!=2 or out2_use or rpl_prefix is not None\n            self.qry_prefix = qry_prefix if qry_prefix is not None else inp_prefix\n            self.rpl_prefix = rpl_prefix if rpl_prefix is not None else out_prefix\n            self.arr_sep = arr_sep\n            self.arr_beg = arr_beg\n            self.arr_end = arr_end\n            self.pretext = pretext\n            self.pre_out = pre_out\n            self.pre_out_empty = ['']*99\n            self.pretext_corpus_split = pretext_corpus_split\n            self.exa_sep = exa_sep\n            self.exa_end = exa_end\n            self.dec_sep = arr_sep if dec_sep is None else dec_sep\n            self.min_wid = min_wid\n            self.min_pad = min_pad\n            self.masking = masking\n            self.collator_kwargs = collator_kwargs\n            self.repeat_input_aug = repeat_input_aug\n            self.repeat_input_pre = repeat_input_pre\n\n        \n    def fmt_array(self, array):\n        return self.arr_beg + self.arr_sep.join(str(row).replace(' ','').replace(',','').replace('[','').replace(']','') + \\\n                                                self.min_pad * max(0, self.min_wid - len(row)) for row in array) + self.arr_end\n\n    def get_pre_out(self, pretext_split):\n        if self.pre_out is None: return self.pre_out_empty\n        #  Input: p = '+/-='\n        # After list(p): ['+', '/', '-', '=']\n        # After adding empty string: ['+', '/', '-', '=', '']\n        # After joining with '\\n': '+\\n/\\n-\\n=\\n'\n        #This formatting is used to create a structured representation of the output markers, where each character is separated by newlines. This is particularly useful when the model needs to process the output markers character by character or when the formatting needs to be consistent with other parts of the text that use the same separator\n        if pretext_split: return [self.pretext_corpus_split.join(list(p) + ['']) for p in self.pre_out]\n        return self.pre_out\n\n    def fmt_train(self, train, last_is_challenge = False, pretext_split = False):\n        po = self.get_pre_out(pretext_split=pretext_split)\n        #This code formats training examples in one of two ways depending on whether it's the last example in a challenge\n        # Output turns into something like this \n        #I123\n        #+/-=\n        #O456\n        ex = [(f\"{self.fmt_query([x], i, pretext_split=pretext_split)}{self.fmt_reply([x['output']])}\" if last_is_challenge and i+1==len(train) else\n               f\"{self.inp_prefix}{self.fmt_array(x['input'])}{self.repeat_input(x, no_aug=pretext_split)}{po[i]}{self.out_prefix}{self.fmt_array(x['output'])}\") for i, x in enumerate(train)]\n        pre = self.pretext_corpus_split.join(list(self.pretext)+['']) if pretext_split else self.pretext\n        end = '' if last_is_challenge else (self.exa_end + self.tokenizer.eos_token)\n        return pre + (self.exa_end + self.tokenizer.eos_token + self.exa_sep).join(ex) + end\n\n    #query = [{'input': [[1, 2, 3], [4, 5, 6]] }] i = 0\n    # qry_prefix = 'I', rpl_prefix = 'O', pre_out = ['+/-='] * 99, arr_sep = '\\n', arr_end = '\\n'\n    # After fmt_array: [[1, 2, 3], [4, 5, 6]] becomes:\n    # \"123\\n456\\n\"\n    # Final formatted text: \"I123\\n456\\n+/-=O\"\n    def fmt_query(self, query, i, pretext_split = False):\n        po = self.get_pre_out(pretext_split = pretext_split)\n        #Takes only the first element of the query list (limits to one example)\n        # Gets the last example and formats it into a query \n        return ''.join(f\"{self.qry_prefix}{self.fmt_array(x['input'])}{self.repeat_input(x, no_aug=pretext_split)}{po[i]}{self.rpl_prefix}\" for x in query[:1])\n\n    def repeat_input(self, x, no_aug = False):\n        if self.repeat_input_aug is None: return ''\n        return f\"{self.repeat_input_pre}{self.fmt_array(((lambda x: x) if no_aug else self.repeat_input_aug)(x['input']))}\"\n\n    def fmt_reply(self, reply, fault = None):\n        ids = self.fmt_array(reply[0]) + self.exa_end + self.tokenizer.eos_token\n        if self.out2_use:\n            if fault is None: fault = reply\n            ids = self.fmt_array(fault[0]) + self.exa_end + self.out2_token + ids\n        return ids\n\n    #Checks for consistant formatting\n    #All segments except possibly the last should be the same length\n    def quick_test(self, decoded, done):\n        sp = decoded.split(self.tokenizer.eos_token)[0].split(self.dec_sep)\n        sl = len(sp[0])\n        is_prefix = sl>0 and len(sp[-1])<=sl and (len(sp)==1 or len(sp[-2])==sl) and all(x.isdigit() for x in sp[-1])\n        return is_prefix and (not done or len(sp[-1])==0 or len(sp[-1])==sl)\n\n    @staticmethod\n    def is_valid_solution(guess):\n        return isinstance(guess, np.ndarray) and guess.ndim == 2 and all(0 < x <= 30 for x in guess.shape)\n\n    def max_new_tokens(self,safety_margin=1):\n        # Create a maximum sized array (30x30) filled with zeros: This represents the largest possible output array (based on ARC challenge constraints)\n        max_sized_reply = np.zeros([30, 30], dtype = int)\n        # Format this array as a reply and tokenize it: Gets the input IDs (token numbers)\n        tokenized = self.tokenizer(self.fmt_reply([max_sized_reply]))['input_ids']\n        # Get the length of the tokenized sequence\n        max_new_tokens = len(tokenized)\n        # Subtracts 1 if it starts with a BOS token (since this isn't part of the new tokens)\n        if tokenized[0]==self.tokenizer.bos_token_id: max_new_tokens -= 1\n        return max_new_tokens + safety_margin\n\n    #IMPORTANT FUNCTION\n    def de_tokenize(self, tokens, scores = None):\n        import torch\n        tokens_cut = cut_at_token(tokens, self.tokenizer.eos_token_id)\n        de_tokenized = self.tokenizer.batch_decode([tokens_cut])[0]\n        score_val = None\n        if scores is not None:\n            tokens_with_eos = tokens[:len(tokens_cut)+1]\n            # Converts raw scores to log probabilities, Selects the probability of each actual token, Sums these probabilities to get total sequence probability\n            # Uses log space for numerical stability, Handles batched inputs efficiently\n            # The result is a score that: Represents how confident the model is in its output Is in log space (typically negative numbers)\n            # Higher values (closer to 0) indicate more confident predictions Lower values (more negative) indicate less confident predictions\n            score_val = torch.nn.functional.log_softmax(torch.tensor(scores), dim=-1).numpy().copy()[np.arange(len(tokens_with_eos)), tokens_with_eos].sum()\n            #Ge the token ids of the numbers\n            number_token_ids = [self.tokenizer.vocab[k] for k in map(str, range(10))]\n            fault_token_id = self.collator_kwargs.get('fault_token_id')\n            if fault_token_id is not None: number_tokens_ids.append(fault_token_id)\n            number_token_ids = np.array(number_token_ids)\n            #for [...,np.newaxis] If tokens_cut is [1, 2, 3, 4, 5] After newaxis: [[1], [2], [3], [4], [5]]\n            # == broadcasts a comparison\n            # Filter scores to only include number tokens\n            number_positions = (tokens_cut[..., np.newaxis] == number_token_ids).any(-1)\n            #Gets the confidence in each number output. Gets rid of other things like newline and stuff\n            score = score[:len(tokens_cut), number_token_ids][number_positions]\n            #Only for positions that contain numbers, In a numerically stable format does\n            # HERE IS WHERE I BELIEVE THERE IS LOG PROB STUFF FROM THE PAPER\n            scores = torch.nn.functional.log_softmax(torch.tensor(scores), dim=-1)[:, :10].numpy().copy()\n        return max(len(tokens) + 1, len(tokens_cut)), score_val, de_tokenized, scores\n\n    #1. Parses text into rows of integers\n    #2. Validates the resulting array, 3. Calculates various score metrics if scores are provided\n    def decode_to_array_single(self, text, score = None, limit_rows = 30):\n        try:\n            by_rows = [row for row in [[int(x) for x in line if x.isdigit()] for line in text.split(self.dec_sep)] if len(row)]\n            if limit_rows and len(by_rows) > limit_rows:\n                by_rows = by_rows[:limit_rows]\n                limited = True\n            else: limited = False\n            decoded = np.array(by_rows, dtype=int)\n            if self.is_valid_solution(decoded):\n                try:\n                    assert score is not None\n                    #Ravel flattens to a single array\n                    decoded_flat = decoded.ravel()\n                    if limited: score = score[:len(decoded_flat)]\n                    # These reshapes are used to shape the scores into decode dimension plus the extra\n                    #dimension of all the score per step\n                    score_all = score.reshape(decoded.shape + score.shape[1:])\n                    score_result = score[range(len(decoded_flat)), decoded_flat]\n                    #converts decoded from numbers into the scores of each number\n                    score_reshape = score_result.reshape(decoded.shape)\n                    # Calculate cumulative sum of scores and reshape to match the original array structure\n                    # This gives us a running total of confidence as we move through the array\n                    score_cum_reshaped = score_result.cumsum().reshape(score_reshaped.shape)\n                    # Calculate cumulative scores for all possible values at each position\n                    # This shows how confidence changes for each possible value compared to the chosen value\n                    score_all_cum = score_cum_reshaped[..., np.newaxis] - score_reshaped[..., np.newaxis] + score_all\n                except: score_reshaped = score_cum_reshaped = np.full(decoded.shape, -float('inf'))\n                return {'output': decoded, 'score': score_reshaped, 'score_cum': score_cum_reshaped, 'score_all': score_all, 'score_all_cum': score_all_cum}\n        except: pass\n        return {}\n\n    # Decodes text into arrays and calculates scores, handling both single and multiple outputs.\n    def decode_to_array(self, text, score=None, limit_rows=30):\n        if not self.out2_use: text, score = [text], [score]\n        else:\n            text = text.split(self.out2_token)\n            if score is None: score = [None]*len(text)\n            else:\n                lengths = np.cumsum([len(list(filter(str.isdigit, t))) for t in text])\n                score = [score[s:e] for s, e in zip([0]+lengths[:-1].tolist(), lengths)]\n        return [self.decode_to_array_single(t, s) for t, s in zip(text, score)]\n\n    #This function is useful for:, Creating a simple training corpus, Testing formatter functionality\n    #Providing basic examples, Ensuring consistent formatting\n    def get_corpus(self):\n        try:\n            old_min_wid, self.min_wid = self.min_wid, min(self.min_wid, 2)\n            return self.fmt_train([{'input': [[i] for i in range(10)], 'output': [[i] for i in range(10)]}]*3, last_is_challenge=True, pretext_split=True)\n        finally: self.min_wid = old_min_wid\n\n    def get_data_collator(self):\n        if not self.masking: return None\n        from transformers import DataCollatorForLanguageModeling\n        collator_params = dict(tokenizer = self.tokenizer, mlm = False)\n        #self.out2_token is a special token used in the masking process for training the language model. It's part of a system that helps the model learn to generate solutions for the ARC challenge.\n        pass_out2_token = self.tokenizer.vocab[self.out2_token] if self.out2_use and self.masking==1 else None\n        if self.masking:\n            assert not self.collator_kwargs.get('mask_first_output') or self.masking == 1\n            data_collator = get_class_MyDataCollator()(\n                **collator_params,\n                instruction_template = [self.inp_prefix, self.tokenizer.bos][self.masking - 1]\n            ).setup(out2_token_id=pass_out2_token, **self.collator_kwargs)\n        else:\n            assert not self.collator_kwargs, 'only supported with masking on'\n            data_collator = DataCollatorForLanguageModeling(**collator_params)\n        return data_collator\n\n    def get_output_token_ids(self):\n        assert not self.out2_use\n        num_tokens = [self.tokenizer.vocab[str(i)] for i in range(10)]\n        sep_tokens = [tok for txt in [self.arr_beg, self.arr_sep, self.arr_end, self.exa_sep] if txt for tok in self.tokenizer(txt)['input_ids'][1:]]\n        sep_tokens.append(self.tokenizer.eos_token_id)\n        return num_tokens + sorted(set(sep_tokens))\n\n    ### CLASS IS NOT DONE BUT I THINK THERE IS ENOUGH TO DISPLAY\n\n\n\nArcFormatter_premix_3 = lambda **kwargs: ArcFormatter(masking = 1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', \\\n                                                      pretext='ABCDEFGHJKLMNPQRSTUVWXYZabcdefghjklmnpqrstuvwxyz', pre_out=['+/-=']*99, \\\n                                                     pretext_corpus_split='\\n', **kwargs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile arc_loader.py\nimport json\nimport numpy as np\nimport hashlib\nimport os, sys\nfrom tqdm import tqdm\nfrom glob import glob\nimport itertools\nimport random\n\ndef cut_at_token(output, token_id):\n    eos_positions = (output==token_id).nonzero()[0]\n    return output[:eos_positions[0]] if len(eos_positions) else output\n\ndef shuffled(data_list):\n    return np.random.permutation(data_list).tolist()\n\ndef permute_mod(a, descriptor, invert=False):\n    permutation = [int(i) for i in descriptor if str(i).isdigit()]\n    assert sorted(permutation)==list(range(10))\n    a = np.asarray(a)\n    if a.ndim==3:\n        if not invert: permutation = np.argsort(permutation)\n        a = a[..., permutation]\n    else:\n        assert a.ndim==2\n        if invert: permutation = np.argsort(permutation)\n        a = np.asarray(permutation)[a]\n    return a\n\ndef permute_rnd_col_(query):\n    permutation = [0]+(1+np.random.permutation(9)).tolist()\n    return 'permute' + ''.join(map(str, permutation))\n\ndef permute_rnd_all_(query):\n    permutation = np.random.permutation(10).tolist()\n    return 'permute' + ''.join(map(str, permutation))\n\ndef permute_cnt_col_(query):\n    elements, frequency = np.unique(np.concatenate([list(range(10))]+[np.array(x['input']).ravel() for x in query['train']]), return_counts=True)\n    permutation = [0]+sorted(np.random.permutation(9)+1, key=lambda i: frequency[i], reverse=True)  # randomness as tie breaker\n    return 'permute' + ''.join(map(str, permutation))\n\ndef permute_cnt_all_(query):\n    elements, frequency = np.unique(np.concatenate([list(range(10))]+[np.array(x['input']).ravel() for x in query['train']]), return_counts=True)\n    permutation = sorted(np.random.permutation(10), key=lambda i: frequency[i], reverse=True)  # randomness as tie breaker\n    return 'permute' + ''.join(map(str, permutation))\n\npermute_rnd_col = (permute_mod, permute_rnd_col_)\npermute_rnd_all = (permute_mod, permute_rnd_all_)\npermute_cnt_col = (permute_mod, permute_cnt_col_)\npermute_cnt_all = (permute_mod, permute_cnt_all_)\npermute_None = (np.copy, None)\n\nclass ArcDataset(object):\n    @staticmethod\n    def forward_mod(a, key, use_perm=True, is_output=True):\n        if a is None: return a\n        for op in key.split('.')[1:]:\n            if op.startswith('I'):\n                if is_output: continue\n                op = op[1:]\n            if   op=='rot90':              a = np.rot90(a)\n            elif op=='transpose':          a = np.swapaxes(a, 0, 1)\n            elif op.startswith('permute'): a = permute_mod(a, op, invert=False) if use_perm else a\n            elif op.startswith('copy'):    a = np.copy(a)\n            elif op.startswith('out'):     a = a\n            elif op.startswith('ex'):      a = a\n            elif op.startswith('fix'):     a = a\n            elif op.startswith('ice'):     a = a  # for adding icecuber solutions\n            else: raise NotImplementedError(f\"Inversion of operation '{op}' unknown.\")\n        return a\n\n    @staticmethod\n    def invert_mod(a, key, inv_perm=True, is_output=True):\n        if a is None: return a\n        for op in key.split('.')[1:][::-1]:\n            if op.startswith('I'):\n                if is_output: continue\n                op = op[1:]\n            if   op=='rot90':              a = np.rot90(np.rot90(np.rot90(a)))\n            elif op=='transpose':          a = np.swapaxes(a, 0, 1)\n            elif op.startswith('permute'): a = permute_mod(a, op, invert=True) if inv_perm else a\n            elif op.startswith('copy'):    a = np.copy(a)\n            elif op.startswith('out'):     a = a\n            elif op.startswith('ex'):      a = a\n            elif op.startswith('fix'):     a = a\n            elif op.startswith('ice'):     a = a  # for adding icecuber solutions\n            else: raise NotImplementedError(f\"Inversion of operation '{op}' unknown.\")\n        return a\n\n    def __init__(self, queries, replies={}, keys=None, is_orig=False, is_fake=False):\n        if keys is not None: keys = [k for k in keys if k is not None]\n        self.queries = queries if keys is None else {k: queries[k] for k in keys}\n        self.replies = replies if keys is None else {k: replies[k] for k in keys if k in replies}\n        self.is_orig = is_orig\n        self.is_fake = is_fake\n        self.keys = sorted(queries.keys()) if keys is None else keys\n        self.faulty = {}\n        self.transposed_dataset = None\n\n    @classmethod\n    def empty(cls):\n        return cls(queries={}, replies={}, keys=[])\n\n    def change_keys(self, keys, keep_flags=False):\n        flags = dict(is_fake=self.is_fake, is_orig=self.is_orig) if keep_flags else {}\n        return self.__class__(queries=self.queries, replies=self.replies, keys=keys, **flags)\n\n    @classmethod\n    def from_file(cls, queries_file):\n        print(f\"*** Load challanges from '{queries_file}'...\")\n        with open(queries_file) as f: queries = f.read()\n        is_fake = hashlib.md5(queries.encode('utf-8')).hexdigest().lower()=='a6b7dac3cab03abf2eb333e16610d6dc'\n        if is_fake: print(\"*** -> Fake test set detected, setting flag 'is_fake' to True.\")\n        return cls(\n            queries=json.loads(queries),\n            is_fake=is_fake,\n            is_orig=True,\n        )\n\n    def load_replies(self, replies_file):\n        print(f\"*** Load solutions from '{replies_file}'...\")\n        with open(replies_file) as f: replies = f.read()\n        replies_parsed = json.loads(replies)\n        self.replies = {k: replies_parsed[k] for k in self.keys}\n        return self\n\n    def split_multi_replies(self):\n        key_indices = [(k, i) for k in self.keys for i in range(len(self.queries[k]['test']))]\n        return self.__class__(\n            keys=[f'{k}_{i}' for k, i in key_indices],\n            queries={f'{k}_{i}': {'train': self.queries[k]['train'], 'test': [self.queries[k]['test'][i]]} for k, i in key_indices},\n            replies={f'{k}_{i}': [self.replies[k][i]] for k, i in key_indices if k in self.replies},\n        )\n\n    def move_test_to_train(self):\n        new_queries = {k: {'train': self.queries[k]['train'] + [{**t, 'output': self.replies[k][i]} for i, t in enumerate(self.queries[k]['test'])], 'test': []} for k in self.keys}\n        return self.__class__(queries=new_queries, keys=[k for k in self.keys])\n\n    def last_train_ex_for_test(self):\n        assert not self.replies\n        new_queries = {k: {'train': self.queries[k]['train'][:-1], 'test': [{'input': self.queries[k]['train'][-1]['input']}]} for k in self.keys}\n        new_replies = {k: [self.queries[k]['train'][-1]['output']] for k in self.keys}\n        return self.__class__(queries=new_queries, replies=new_replies, keys=[k for k in self.keys])\n\n    def length(self):\n        return len(self.keys)\n\n    def shuffled(self, seed=None):\n        if seed is not None: np.random.seed(seed)\n        return self.__class__(queries=self.queries, replies=self.replies, keys=shuffled(self.keys))\n\n    def sorted(self, **kwargs):\n        return self.__class__(queries=self.queries, replies=self.replies, keys=sorted(self.keys, **kwargs))\n\n    def append(*datasets):\n        return datasets[0].__class__(\n            queries={k: v for d in datasets for k, v in d.queries.items()},\n            replies={k: v for d in datasets for k, v in d.replies.items()},\n            keys   =[k    for d in datasets for k    in d.keys           ],\n        )\n\n    def sort_ex_by_input_size(self, seed=42, reverse=False):\n        np.random.seed(seed)\n        sort_key = lambda ex: np.prod(np.shape(ex['input']))\n        new_queries = {k2: {k: (sorted(np.random.permutation(np.array(v, dtype=object)), key=sort_key, reverse=reverse) if k=='train' else v) for k, v in v2.items()} for k2, v2 in self.queries.items()}\n        return self.__class__(queries=new_queries, replies=self.replies, keys=[k for k in self.keys])\n\n    def interleave(self, block_size, num_gpus=None):\n        keys = np.reshape(self.keys, (-1, block_size)).T\n        if num_gpus is None: return self.change_keys(keys.ravel().tolist())\n        ret, num_gpus = (None, num_gpus) if isinstance(num_gpus, int) else num_gpus\n        keys = np.concatenate([keys, np.full((-keys.shape[0]%num_gpus, keys.shape[1]), None)])\n        keys = np.reshape(keys, (keys.shape[0]//num_gpus, num_gpus, -1)).swapaxes(0, 1).reshape(num_gpus, -1)\n        new_datasets = [self.change_keys(gpu_keys.tolist()) for gpu_keys in keys]\n        return new_datasets if ret is None else new_datasets[ret]\n\n    def remove(self, *datasets):\n        remove_keys = {k for d in datasets for k in d.keys}\n        new_keys = [k for k in self.keys if k not in remove_keys]\n        return self.change_keys(new_keys)\n\n    def keep_key_startswith(self, key_start):\n        new_keys = [k for k in self.keys if k.startswith(key_start)]\n        return self.change_keys(new_keys)\n\n    def mod_single(self, mod_func, descriptor, i, keep_key, inputs_only):\n        queries = {}\n        replies = {}\n        keys    = []\n        for k0 in self.keys:\n            desc = (('copy{i}' if mod_func is np.copy else mod_func.__name__) if descriptor is None else descriptor if isinstance(descriptor, str) else descriptor(self.queries[k0])).format(i=i)\n            func = lambda a, d: np.asarray(mod_func(a) if descriptor is None else mod_func(a, d)).tolist()\n            k1 = k0 if keep_key else f\"{k0}.{'I' if inputs_only else ''}{desc}\"\n            keys.append(k1)\n            queries[k1] = {m: [{t: (func(a, desc) if t=='input' or not inputs_only else a) for t, a in x.items()} for x in e] for m, e in self.queries[k0].items()}\n            if k0 in self.replies:\n                replies[k1] = [func(a, desc) for a in self.replies[k0]]\n        ret = self.__class__(queries=queries, replies=replies, keys=keys)\n        return ret\n\n    def mod(self, mod_func, descriptor=None, n=1, stack=None, keep=False, keep_key=False, shuffle=False, join=True, inputs_only=False):\n        assert not (keep and keep_key)\n        cur = self\n        ret = [cur.shuffled() if shuffle else cur] if keep else []\n        if stack is None: stack = mod_func.__name__.startswith('rot')\n        for i in range(n):\n            cur = (cur if stack else self).mod_single(mod_func, descriptor, i=i, keep_key=keep_key, inputs_only=inputs_only)\n            ret.append(cur.shuffled() if shuffle else cur)\n        return self.__class__.append(*ret) if join else ret\n\n    def get(self, key, formatter):\n        assert formatter.out2_token is None or key in self.replies\n        train = formatter.fmt_train(self.queries[key]['train'])\n        query = formatter.fmt_query(self.queries[key]['test'], i=len(self.queries[key]['train']))\n        reply = formatter.fmt_reply(self.replies[key], self.faulty.get(key)) if key in self.replies else ''\n        text = train+query+reply if reply else formatter.fmt_train(self.queries[key]['train'], last_is_challenge=True)\n        return dict(key=key, train=train, query=query, reply=reply, input=train+query, text=text)\n\n    def as_list(self, formatter):\n        return [self.get(key, formatter) for key in self.keys]\n\n    def as_dataset(self):\n        from datasets import Dataset\n        return Dataset.from_list([{'key': k, 'query': self.queries[k], 'reply': self.replies[k]} for k in self.keys])\n\n    def get_length(self, key, formatter, name, max_of_transposed=False):\n        if formatter is None:\n            if   name=='input': return sum(np.prod(np.shape(v)) for v3 in self.queries[key].values() for v2 in v3 for v in v2.values())\n            elif name=='reply': return sum(np.prod(np.shape(v)) for v in self.replies[key])\n            else: assert False\n        else:\n            datasets = [self]\n            if max_of_transposed:\n                if self.transposed_dataset is None: self.transposed_dataset = self.mod(np.transpose, keep=False, keep_key=True)\n                datasets.append(self.transposed_dataset)\n            return max(len(formatter.tokenizer(ds.get(key, formatter=formatter)[name])['input_ids']) for ds in datasets)\n\n    def get_lengths(self, formatter, name, max_of_transposed=False):\n        return {key: self.get_length(key, formatter=formatter, name=name, max_of_transposed=max_of_transposed) for key in self.keys}\n\n    def sorted_by_len(self, reverse=False, **kwargs):\n        new_keys = [key for _, key in sorted([(v, k) for k, v in self.get_lengths(**kwargs).items()], reverse=reverse)]\n        return self.change_keys(new_keys)\n\n    def filter_by_len(self, min_len=0, max_len=float('inf'), **kwargs):\n        new_keys = [k for k, v in self.get_lengths(**kwargs).items() if min_len<=v<=max_len]\n        return self.change_keys(new_keys)\n\n    def cut_to_query_count(self, max_count, from_end=False):\n        new_queries = {}\n        for k in self.keys:\n            new_queries[k] = q = self.queries[k]\n            while len(q['train'])>max_count: q['train'] = q['train'][:-1] if from_end else q['train'][1:]\n        return self.__class__(queries=new_queries, replies=self.replies, keys=[k for k in self.keys])\n\n    def cut_to_len(self, formatter, name, max_len, max_new_tokens='auto', from_end=False, quiet=False, **kwargs):\n        if max_new_tokens:\n            if max_new_tokens=='auto': max_new_tokens = formatter.max_new_tokens()\n            max_len_old, max_len = max_len, max_len - max_new_tokens\n            if not quiet: print(f'*** Reducing task size to max. {max_len_old} tokens ({max_len} input + {max_new_tokens} generated)...')\n        elif not quiet: print(f'*** Reducing task size to max. {max_len} tokens...')\n        temp_ds = self.change_keys(self.keys)\n        new_keys = []\n        new_queries = {}\n        new_replies = {}\n        for key in (self.keys if quiet else tqdm(self.keys, file=sys.stdout)):\n            reply = temp_ds.replies.get(key)\n            while max_len<temp_ds.get_length(key, formatter=formatter, name=name, **kwargs):\n                query = temp_ds.queries[key]\n                if not key.split('.')[-1].startswith('ex'): key = f\"{key}.ex{''.join(map(str, range(len(query['train']))))}\"\n                key_split = key.split('.')\n                assert key_split[-1].startswith('ex')\n                key = '.'.join(key_split[:-1] + [f'ex{key_split[-1][2:-1] if from_end else key_split[-1][3:]}'])\n                temp_ds.queries[key] = {k: ((v[:-1] if from_end else v[1:]) if k=='train' else v) for k, v in query.items()}\n                if reply is not None: temp_ds.replies[key] = reply\n            new_keys.append(key)\n            new_queries[key] = temp_ds.queries[key]\n            if reply is not None: new_replies[key] = reply\n        return self.__class__(keys=new_keys, queries=new_queries, replies=new_replies)\n\n    def shuffle_ex(self, perm=None, keep_max=None):\n        new_keys = []\n        new_queries = {}\n        new_replies = {}\n        for key in self.keys:\n            n = len(self.queries[key]['train'])\n            p = np.random.permutation(n) if perm is None else perm\n            if keep_max is not None: p = p[:keep_max]\n            new_key = f'{key}.ex' + ('-' if (p.max()>9) else '').join(map(str, p.tolist()))\n            new_keys.append(new_key)\n            new_queries[new_key] = {k: (np.array(v, dtype=object)[p].tolist() if k=='train' else v) for k, v in self.queries[key].items()}\n            if key in self.replies: new_replies[new_key] = self.replies[key]\n        return self.__class__(queries=new_queries, replies=new_replies, keys=new_keys)\n\n    def shuffle_rp(self, keep_max=None):\n        new_keys = []\n        new_queries = {}\n        new_replies = {}\n        for key in self.keys:\n            n = len(self.queries[key]['test'])\n            p = np.random.permutation(n)\n            if keep_max is not None: p = p[:keep_max]\n            new_key = f'{key}.rp' + ('-' if (p.max()>9) else '').join(map(str, p.tolist()))\n            new_keys.append(new_key)\n            new_queries[new_key] = {k: (np.array(v, dtype=object)[p].tolist() if k=='test' else v) for k, v in self.queries[key].items()}\n            if key in self.replies: new_replies[new_key] = np.array(self.replies[key], dtype=object)[p].tolist()\n        return self.__class__(queries=new_queries, replies=new_replies, keys=new_keys)\n\n    def append_to_keys(self, test):\n        return self.change_keys([f'{k}{text}' for k in self.keys])\n\n    def random_select(self, n):\n        keys = np.array(self.keys).reshape(n, -1).T\n        choice = np.random.randint(0, n, size=[len(keys)])\n        return self.change_keys(keys[np.arange(len(keys)), choice])\n\n    def augment(self, tp=False, rot=False, n=1, perm=None, perm_append=False, shfl_keys=False, shfl_ex=False, seed=None, quiet=False, inputs_only=False):\n        if not quiet: print(f\"*** Augment dataset{' (inputs only)' if inputs_only else ''}...\")\n        np.random.seed(seed)\n        d = self\n        if tp: d = d.mod(np.transpose, keep=True, inputs_only=inputs_only)\n        if tp=='rand': d = d.random_select(n=2)\n        if rot: d = d.mod(np.rot90, n=3, keep=True, inputs_only=inputs_only)\n        if rot=='rand': d = d.random_select(n=4)\n        if perm is None and n<=1: d = d.shuffled() if shfl_keys else d\n        else: d = d.mod(*([np.copy] if perm is None else globals()[f\"permute_{perm}\"]), n=n, shuffle=shfl_keys, keep=perm_append, inputs_only=inputs_only)\n        np.random.seed(seed)\n        if shfl_ex: d = d.shuffle_ex()\n        return d\n\n    def remove_replies(self):\n        return self.__class__(queries=self.queries, replies={}, keys=[k for k in self.keys])\n\n    def split_at_pos(self, pos, random_seed=None):\n        keys = self.keys\n        if random_seed is not None:\n            np.random.seed(random_seed)\n            keys = np.random.permutation(keys)\n        if isinstance(pos, float): pos = int(pos * len(self.keys) + 0.5)\n        keys_split = [keys[:pos], keys[pos:]]\n        return tuple(self.change_keys(new_keys, keep_flags=True) for new_keys in keys_split)\n\n    def get_submission(self, results=None):\n        assert self.is_orig==True, 'Must be run on original dataset.'\n        submission = {k: [{f'attempt_{i+1}': [[0]] for i in range(2)} for _ in range(len(self.queries[k]['test']))] for k in self.keys}\n        if results is not None: self.fill_submission(results, submission)\n        return submission\n\n    @staticmethod\n    def fill_submission(results, submission):\n        print(f'*** Generating submission for {len(results)} outputs...')\n        for k, v in results.items():\n            base_id, base_nr = k.split('_')\n            target_dict = submission[base_id][int(base_nr)]\n            for i, g in enumerate(v[:len(target_dict)]):\n                target_dict[f'attempt_{i+1}'] = g.tolist()\n\n    def validate_submission(self, submission):\n        assert self.is_orig==True, 'Must be run on original dataset.'\n        score = 0\n        for k, v in self.replies.items():\n            for i, r in enumerate(v):\n                for attempt in ['attempt_1', 'attempt_2']:\n                    if np.array_equal(r, submission[k][i][attempt]):\n                        score += 1 / len(v)\n                        break\n        return score\ndef get_class_MyDataCollator(cache=[]):\n    if not cache:\n        from trl import DataCollatorForCompletionOnlyLM\n        class MyDataCollator(DataCollatorForCompletionOnlyLM):\n            def setup(self, out2_token_id=None, fault_token_id=None, fault_freq=0, sample_tries=8, mask_first_output=False):                \n                self.out2_token_id = out2_token_id                \n                self.fault_token_id = fault_token_id\n                self.fault_freq = fault_freq\n                self.sample_tries = sample_tries\n                self.mask_first_output = mask_first_output\n                return self\n\n            def torch_call(self, examples):\n                print(f\">>> [Debug Collator] Entering torch_call. Number of examples: {len(examples)}\")\n                batch = super().torch_call(examples)\n                print(f\">>> [Debug Collator] super().torch_call returned. Batch keys: {batch.keys()}\")\n                if self.out2_token_id is not None:\n                    assert not self.fault_freq\n                    for i in range(len(batch['input_ids'])):\n                        end_pos = ((batch['labels'][i] != -100              ).nonzero().max()).item() + 1\n                        mid_pos = ((batch['labels'][i] == self.out2_token_id).nonzero().max()).item() + 1\n                        beg_pos = mid_pos - (end_pos - mid_pos)\n                        batch['labels'][i][beg_pos:mid_pos] = batch['labels'][i][mid_pos:end_pos]\n                elif self.fault_freq:\n                    for i in range(len(batch['input_ids'])):\n                        end_pos = ((batch['labels'][i] != -100).nonzero().max()).item() + 1\n                        if not isinstance(self.fault_freq, float):\n                            eos_token_id = batch['labels'][i][end_pos - 1]\n                            num_examples = (batch['labels'][i] == eos_token_id).sum().item() - 1\n                            fault_freq = self.fault_freq[num_examples]\n                        else: fault_freq = self.fault_freq\n                        if random.random() < fault_freq:\n                            beg_pos = ((batch['labels'][i][:end_pos]==-100).nonzero().max()).item() + 1\n                            fault_pos = random.randint(beg_pos, end_pos-2)\n                            fault_tok = batch['labels'][i][fault_pos].item()\n                            for t in range(self.sample_tries):\n                                new_tok = batch['labels'][i][random.randint(beg_pos, end_pos-2)].item()\n                                if fault_tok!=new_tok:\n                                    batch['input_ids'][i][fault_pos] = new_tok\n                                    batch['labels'][i][fault_pos+1:end_pos] = self.fault_token_id\n                                    break\n                for i in range(len(batch['labels'])):\n                    for _ in range(self.mask_first_output):\n                        beg_pos = ((batch['labels'][i] != -100).nonzero().min()).item()\n                        mid_pos = ((batch['labels'][i][beg_pos:] == -100).nonzero().min()).item() + beg_pos\n                        end_pos = ((batch['labels'][i] != -100).nonzero().max()).item() + 1\n                        if mid_pos<end_pos: batch['labels'][i][beg_pos:mid_pos] = -100\n                return batch\n        cache.append(MyDataCollator)\n    return cache[0]\n\nclass ArcFormatter(object):\n    def __init__(self, inp_prefix, out_prefix, arr_sep, out2_use=False, out2_token=None, arr_beg='', arr_end='', pretext='', pre_out=None, exa_sep='', exa_end='', qry_prefix=None, rpl_prefix=None, rpl_sep=None, dec_sep=None, min_wid=0, min_pad='', pretext_corpus_split='', masking=0, tokenizer=None, collator_kwargs={}, repeat_input_aug=None, repeat_input_pre=None):\n        self.tokenizer = tokenizer\n        self.inp_prefix = inp_prefix\n        self.out_prefix = out_prefix\n        self.out2_token = out2_token\n        self.out2_use = out2_use\n        assert not out2_use or out2_token is not None\n        assert not out2_use or masking in [1, 2]\n        assert masking!=2 or out2_use or rpl_prefix is not None\n        self.qry_prefix = qry_prefix if qry_prefix is not None else inp_prefix\n        self.rpl_prefix = rpl_prefix if rpl_prefix is not None else out_prefix\n        self.rpl_sep = rpl_sep if rpl_sep is not None else self.rpl_prefix\n        self.arr_sep = arr_sep\n        self.arr_beg = arr_beg\n        self.arr_end = arr_end\n        self.pretext = pretext\n        self.pre_out = pre_out\n        self.pre_out_empty = ['']*99\n        self.pretext_corpus_split = pretext_corpus_split\n        self.exa_sep = exa_sep\n        self.exa_end = exa_end\n        self.dec_sep = arr_sep if dec_sep is None else dec_sep\n        self.min_wid = min_wid\n        self.min_pad = min_pad\n        self.masking = masking\n        self.collator_kwargs = collator_kwargs\n        self.repeat_input_aug = repeat_input_aug\n        self.repeat_input_pre = repeat_input_pre\n\n    def fmt_array(self, array):\n        return self.arr_beg + self.arr_sep.join(str(row).replace(' ', '').replace(',', '').replace('[', '').replace(']', '')+self.min_pad*max(0, self.min_wid-len(row)) for row in array) + self.arr_end\n\n    def get_pre_out(self, pretext_split):\n        if self.pre_out is None: return self.pre_out_empty\n        if pretext_split: return [self.pretext_corpus_split.join(list(p) + ['']) for p in self.pre_out]\n        return self.pre_out\n\n    def fmt_train(self, train, last_is_challenge=False, pretext_split=False):\n        po = self.get_pre_out(pretext_split=pretext_split)\n        ex = [(f\"{self.fmt_query([x], i, pretext_split=pretext_split)}{self.fmt_reply([x['output']])}\" if last_is_challenge and i+1==len(train) else\n               f\"{self.inp_prefix}{self.fmt_array(x['input'])}{self.repeat_input(x, no_aug=pretext_split)}{po[i]}{self.out_prefix}{self.fmt_array(x['output'])}\") for i, x in enumerate(train)]\n        pre = self.pretext_corpus_split.join(list(self.pretext)+['']) if pretext_split else self.pretext\n        end = '' if last_is_challenge else (self.exa_end + self.tokenizer.eos_token)\n        return pre + (self.exa_end + self.tokenizer.eos_token + self.exa_sep).join(ex) + end\n\n    def fmt_query(self, query, i, pretext_split=False):\n        po = self.get_pre_out(pretext_split=pretext_split)\n        return ''.join(f\"{self.qry_prefix}{self.fmt_array(x['input'])}{self.repeat_input(x, no_aug=pretext_split)}{po[i]}{self.rpl_prefix}\" for x in query[:1])\n\n    def repeat_input(self, x, no_aug=False):\n        if self.repeat_input_aug is None: return ''\n        return f\"{self.repeat_input_pre}{self.fmt_array(((lambda x: x) if no_aug else self.repeat_input_aug)(x['input']))}\"\n\n    def fmt_reply(self, reply, fault=None):\n        ids = self.fmt_array(reply[0]) + self.exa_end + self.tokenizer.eos_token\n        if self.out2_use:\n            if fault is None: fault = reply\n            ids = self.fmt_array(fault[0]) + self.exa_end + self.out2_token + ids\n        return ids\n\n    def quick_test(self, decoded, done):\n        sp = decoded.split(self.tokenizer.eos_token)[0].split(self.dec_sep)\n        sl = len(sp[0])\n        is_prefix = sl>0 and len(sp[-1])<=sl and (len(sp)==1 or len(sp[-2])==sl) and all(x.isdigit() for x in sp[-1])\n        return is_prefix and (not done or len(sp[-1])==0 or len(sp[-1])==sl)\n\n    @staticmethod\n    def is_valid_solution(guess):\n        return isinstance(guess, np.ndarray) and guess.ndim == 2 and all(0 < x <= 30 for x in guess.shape)\n\n    def max_new_tokens(self, safety_margin=1):\n        max_sized_reply = np.zeros([30, 30], dtype=int)\n        tokenized = self.tokenizer(self.fmt_reply([max_sized_reply]))['input_ids']\n        max_new_tokens = len(tokenized)\n        if tokenized[0]==self.tokenizer.bos_token_id: max_new_tokens -= 1\n        return max_new_tokens + safety_margin\n\n    def de_tokenize(self, tokens, scores=None):\n        import torch\n        tokens_cut = cut_at_token(tokens, self.tokenizer.eos_token_id)\n        de_tokenized = self.tokenizer.batch_decode([tokens_cut])[0]\n        score_val = None\n        if scores is not None:\n            tokens_with_eos = tokens[:len(tokens_cut)+1]\n            score_val = torch.nn.functional.log_softmax(torch.tensor(scores), dim=-1).numpy().copy()[np.arange(len(tokens_with_eos)), tokens_with_eos].sum()\n            number_token_ids = [self.tokenizer.vocab[k] for k in map(str, range(10))]\n            fault_token_id = self.collator_kwargs.get('fault_token_id')\n            if fault_token_id is not None: number_token_ids.append(fault_token_id)\n            number_token_ids = np.array(number_token_ids)\n            number_positions = (tokens_cut[..., np.newaxis] == number_token_ids).any(-1)\n            scores = scores[:len(tokens_cut), number_token_ids][number_positions]\n            scores = torch.nn.functional.log_softmax(torch.tensor(scores), dim=-1)[:, :10].numpy().copy()\n        return max(len(tokens)+1, len(tokens_cut)), score_val, de_tokenized, scores\n\n    def decode_to_array_single(self, text, score=None, limit_rows=30):\n        try:\n            by_rows = [row for row in [[int(x) for x in line if x.isdigit()] for line in text.split(self.dec_sep)] if len(row)]\n            if limit_rows and len(by_rows) > limit_rows:\n                by_rows = by_rows[:limit_rows]\n                limited = True\n            else: limited = False\n            decoded = np.array(by_rows, dtype=int)\n            if self.is_valid_solution(decoded):\n                try:\n                    assert score is not None\n                    decoded_flat = decoded.ravel()\n                    if limited: score = score[:len(decoded_flat)]\n                    score_all = score.reshape(decoded.shape + score.shape[1:])\n                    score_result = score[range(len(decoded_flat)), decoded_flat]\n                    score_reshaped = score_result.reshape(decoded.shape)\n                    score_cum_reshaped = score_result.cumsum().reshape(score_reshaped.shape)\n                    score_all_cum = score_cum_reshaped[..., np.newaxis] - score_reshaped[..., np.newaxis] + score_all\n                except: score_reshaped = score_cum_reshaped = np.full(decoded.shape, -float('inf'))\n                return {'output': decoded, 'score': score_reshaped, 'score_cum': score_cum_reshaped, 'score_all': score_all, 'score_all_cum': score_all_cum}\n        except: pass\n        return {}\n\n    def decode_to_array(self, text, score=None, limit_rows=30):\n        if not self.out2_use: text, score = [text], [score]\n        else:\n            text = text.split(self.out2_token)\n            if score is None: score = [None]*len(text)\n            else:\n                lengths = np.cumsum([len(list(filter(str.isdigit, t))) for t in text])\n                score = [score[s:e] for s, e in zip([0]+lengths[:-1].tolist(), lengths)]\n        return [self.decode_to_array_single(t, s) for t, s in zip(text, score)]\n\n    def get_corpus(self):\n        try:\n            old_min_wid, self.min_wid = self.min_wid, min(self.min_wid, 2)\n            return self.fmt_train([{'input': [[i] for i in range(10)], 'output': [[i] for i in range(10)]}]*3, last_is_challenge=True, pretext_split=True)\n        finally: self.min_wid = old_min_wid\n\n    def get_data_collator(self):\n        if not self.masking: return None\n        from transformers import DataCollatorForLanguageModeling\n        collator_params = dict(tokenizer=self.tokenizer, mlm=False)\n        pass_out2_token = self.tokenizer.vocab[self.out2_token] if self.out2_use and self.masking==1 else None\n        if self.masking:\n            assert not self.collator_kwargs.get('mask_first_output') or self.masking==1\n            data_collator = get_class_MyDataCollator()(\n                **collator_params,\n                instruction_template=[self.inp_prefix, self.tokenizer.bos_token][self.masking - 1],\n                response_template=[self.out_prefix, (self.out2_token if self.out2_use else self.rpl_sep)][self.masking - 1],\n            ).setup(out2_token_id=pass_out2_token, **self.collator_kwargs)\n        else:\n            assert not self.collator_kwargs, 'only supported with masking on'\n            data_collator = DataCollatorForLanguageModeling(**collator_params)\n        return data_collator\n\n    def get_output_token_ids(self):\n        assert not self.out2_use\n        num_tokens = [self.tokenizer.vocab[str(i)] for i in range(10)]\n        sep_tokens = [tok for txt in [self.arr_beg, self.arr_sep, self.arr_end, self.exa_sep] if txt for tok in self.tokenizer(txt)['input_ids'][1:]]\n        sep_tokens.append(self.tokenizer.eos_token_id)\n        return num_tokens + sorted(set(sep_tokens))\n\nArcFormatter_pretext2 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZ', pretext_corpus_split='\\n', **kwargs)\nArcFormatter_pretext3 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZabcdefghjklmnpqrstuvwxyz', pretext_corpus_split='\\n', **kwargs)\nArcFormatter_premix_2 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZ', pre_out=['+/-=']*99, pretext_corpus_split='\\n', **kwargs)\nArcFormatter_premix_3 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZabcdefghjklmnpqrstuvwxyz', pre_out=['+/-=']*99, pretext_corpus_split='\\n', **kwargs)\n\navailable_formatters = dict(\n    ArcFormatter_pretext2=ArcFormatter_pretext2,\n    ArcFormatter_pretext3=ArcFormatter_pretext3,\n    ArcFormatter_premix_2=ArcFormatter_premix_2,\n    ArcFormatter_premix_3=ArcFormatter_premix_3,\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-04-11T02:22:40.825420Z","iopub.execute_input":"2025-04-11T02:22:40.825695Z","iopub.status.idle":"2025-04-11T02:22:40.846056Z","shell.execute_reply.started":"2025-04-11T02:22:40.825674Z","shell.execute_reply":"2025-04-11T02:22:40.845328Z"}},"outputs":[{"name":"stdout","text":"Overwriting arc_loader.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile model_runner.py\n\nimport json\nimport os, sys\nimport bz2\nimport pickle\nimport numpy as np\nfrom tqdm import tqdm\n\n# This code is important because it: # Removes temporary training artifacts\n# Ensures clean weight loading # Prevents memory leaks from unused weights\n# Maintains compatibility with the model architecture\ndef get_and_fix_peft_weights(store):\n    print(f\"*** Load peft state_dict from '{store}'...\")\n    from peft import load_peft_weights\n    state_dict = load_peft_weights(store)\n    for k in list(state_dict.keys()):\n        if 'modules_to_save' in k:\n            del state_dict[k]\n            original_module_key = k.replace('.modules_to_save.', '.original_module.')\n            if original_module_key in state_dict: del state_dict[original_module_key]\n            assert k.replace('.modules_to_save.', '.') in state_dict\n    return state_dict\n\ndef set_peft_weights(model, state_dict):\n    print(f\"*** Set model state_dict...\")\n    from peft import set_peft_model_state_dict\n    res = set_peft_model_state_dict(model, state_dict)\n    assert not res.unexpected_keys\n\ndef load_peft_state(model, store):\n    set_peft_weights(model, get_and_fix_peft_weights(store))\n\ndef is_peft_model(model):\n    return hasattr(model, 'peft_type')\n\n# I NEED TO UNDERSTAND THIS PEFT STUFF BETTER\n\n# I SET tf_use_fa2 from True to False\ndef prepare_model(model, mode, tokenizer=None, formatter=None, shrink_embedding=False, \\\n                  dequantize=False, peft=[], local_files_only=False, add_special_tokens={}, \\\n                  set_pad_token=None, keep_tokens=[], keep_normalizer=None, peft_trainable=True, \\\n                  device_map=None, tf_grad_cp=True, tf_use_fa2=False, **kwargs):\n    if isinstance(model, str):\n        assert tokenizer is None\n        print(f\"*** Load base model and tokenizer from '{model}'...\")\n        if mode in ['transformers', 'transformers_bf16', 'transformers_4bit', 'transformers_bf16_4bit', 'tokenizer_only']:\n            import torch\n            model_load_args = {}\n            #The device_map tells the model which parts should go on which device\n            if device_map is not None: model_load_args['device_map'] = device_map\n            if tf_use_fa2: model_load_args['attn_implementation'] = 'flash_attention_2'\n            if mode in ['transformers_bf16', 'transformers_bf16_4bit']: model_load_args['torch_dtype'] = torch.bfloat16\n            elif mode in ['transformers_4bit', 'transformers_bf16_4bit']:\n                from transformers import BitsAndBytesConfig\n                nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n                model_load_args['quantization_config'] = nf4_config\n            from transformers import AutoTokenizer, AutoModelForCausalLM\n            tokenizer = AutoTokenizer.from_pretrained(model, local_files_only=local_files_only, **kwargs)\n            model = AutoModelForCausalLM.from_pretrained(model, **model_load_args) if mode!='tokenizer_only' else None\n            if tf_grad_cp and model is not None: model.gradient_checkpointing_enable()\n        else: raise NotImplementedError('Unknown mode.')\n    if add_special_tokens: tokenizer.add_special_tokens(add_special_tokens)\n    if set_pad_token is not None: tokenizer.pad_token = set_pad_token\n    if formatter is not None and not hasattr(formatter, 'corpus'):\n        formatter = formatter(tokenizer=tokenizer)\n    #The purpose of this line appears to be determining when to shrink the model's embedding layer. This would happen if either:\n    #The specified shrink size is smaller than the current vocabulary size OR if we explicitly want to remove the normalizer\n    if (shrink_embedding<len(tokenizer.vocab) if type(shrink_embedding)==int else shrink_embedding) or keep_normalizer is False:\n        print(\"*** Shrunk embedding...\")\n        embedding_size_before_shrink = len(tokenizer.vocab)\n        #It keeps only the tokens that are actually needed for your specific use case\n        mapping = shrink_embeddings(model, tokenizer, formatter.get_corpus(), keep_tokens = keep_tokens, \\\n                                   keep_normalizer = keep_normalizer)\n        print(f'*** -> Reduced embedding size from {embedding_size_before_shrink} to {len(mapping)} words.')\n    if len(peft):\n        peft_trained = True if is_peft_model(model) else None\n        for i, m in enumerate(peft):\n            # PROPERLY LOADS THE MODEL THAT WAS TRAINED USING PEFT\n            if peft_trained is True: model, peft_trained = merge_peft_into_base(model),\n            # We can use this to create our own peft config if we want to make a new one\n            # We don't have to if we are pre-training with Lora\n            if isinstance(m, str):\n                if peft_trained is False:\n                    _, peft_trained = load_peft_state(model, m), True\n                else:\n                    print(f\"*** Load peft model from '{m}'...\")\n                    from peft import PeftModel\n                    model, peft_trained = PeftModel.from_pretrained(model, m, trainable = peft_trainable), True\n            else:\n                assert peft_trained is None\n                if isinstance(m, dict):\n                    print('*** Create new peft model...')\n                    from peft import LoraConfig, get_peft_model\n                    my_get_peft_model = lambda model, **kwargs: get_peft_model(model, LoraConfig(**kwargs))  \n                    model, peft_trained = my_get_peft_model(model, **m), False\n                else: \n                    assert m is None\n    return model, tokenizer, formatter\n\n\ndef training_run(model, formatter, dataset, train_args, max_seq_length, merge=False, store=None, \\\n                 packing=False, grad_acc_fix=False, optimizers=None):\n    # NOT SURE IF WE NEED THIS???\n    assert merge is False, \"merge after training does not seen to work (at least with unsloth, saved merged model will contain the untrained weights!)\"\n    import torch\n    from datasets import Dataset\n    add_train_args = {}\n    from trl import SFTConfig as TrainingArguments\n    # LOOK INTO UNDERSTANDING THIS\n    from trl import SFTTrainer as Trainer\n    model.train()\n    add_train_args.update(bf16 = True)\n\n    formatter.tokenizer.padding_side = 'right'\n\n    add_args = {}\n    if optimizers is not None: add_args['optimzers'] = optimizers\n\n    trainer = Trainer(\n        model=model,\n        tokenizer=formatter.tokenizer,\n        data_collator=formatter.get_data_collator(),\n        train_dataset=Dataset.from_list(dataset.as_list(formatter)),\n        dataset_text_field=\"text\",\n        max_seq_length=max_seq_length,\n        dataset_num_proc=None,\n        packing=packing, # Can make training 5x faster for short sequences.\n        **add_args,\n        args=TrainingArguments(\n            **add_train_args,\n            **train_args\n        ),\n    )\n    \n    print('*** Start training run...')\n    \n    trainer_stats = trainer.train()\n    try: print(f'*** -> Training took {trainer_stats.metrics[\"train_runtime\"]} seconds.')\n    except: print(\"Couldn't print trainer stats metrics train runtime\")\n    if store is not None: save_model(store, model, formatter.tokenizer, merge=merge)\n    return model, trainer_stats\n\n        \n\n\nclass Retrainer(object):\n    def __init__(self, n, aug_opts, reload_state_dict = None, **kwargs):\n        self.n = n\n        self.aug_opts = aug_opts\n        self.reload_state_dict = reload_state_dict\n        self.kwargs  = kwargs\n\n    def preprocess(self, dataset):\n        #Creates multiple augmented versions of the dataset\n        #self.n: The total number of examples you want dataset.length(): Current size of the dataset\n        #The formula calculates how many times we need to augment the dataset to reach self.n examples\n        ds = [dataset.augment(quiet=True, shfl_keys = True, **self.aug_opts) for _ in range((self.n-1)//dataset.length()+1)]\n        # If there's only one augmented dataset (len(ds)==1): just use the single dataset\n        #if there are multiple append them to the dataset\n        ds = ds[0] if len(ds)==1 else ds[0].append(*ds[1:])\n        #Ensures we only have n samples in the dataset\n        ds, _ = ds.split_at_pos(self.n)\n\n    def __call__(self, model, dataset):\n        if self.reload_state_dict is not None: set_peft_weights(model, self.reload_state_dict)\n        model.train()\n        training_run(model, dataset = self.preprocess(dataset), **self.kwargs)\n        \n\n# class Decoder(object):\n#     def __init__(self, formatter, dataset, n_guesses, max_outputs = None, frac_score = False, quiet = False, name='', additional_decoders=None, prob_baseline=None):\n#         self.formatter = formatter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T02:22:44.012087Z","iopub.execute_input":"2025-04-11T02:22:44.012363Z","iopub.status.idle":"2025-04-11T02:22:44.020457Z","shell.execute_reply.started":"2025-04-11T02:22:44.012333Z","shell.execute_reply":"2025-04-11T02:22:44.019677Z"}},"outputs":[{"name":"stdout","text":"Overwriting model_runner.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import time\nfrom arc_loader import *\nimport os\nbase_path, running_on_kaggle = ('/kaggle', True) if os.path.exists('/kaggle') else ('.', False)\n\narc_challenge_file = os.path.join(base_path, 'input', 'arc-prize-2024', 'arc-agi_evaluation_challenges.json')\nprint(arc_challenge_file)\n# load datasets\narc_test_set = ArcDataset.from_file(arc_challenge_file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# Define the original JSON file name\nfilename = \"/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json\"\n\n# Step 1: Load the JSON file\nwith open(filename, 'r') as file:\n    data = json.load(file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_data = {}\nnew_data['00576224'] = data['00576224'] #, '009d5c81', '00dbd492']\nnew_data['009d5c81'] = data['009d5c81']\nnew_data['00dbd492'] = data['00dbd492']\n# print(new_data)\nnew_filename = \"/kaggle/working/sub_arc-agi_evaluation_challenges.json\"\nwith open(new_filename, 'w') as file:\n    json.dump(new_data, file, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip uninstall --yes torch\n#! pip install \"torch==2.4.1\"\n!pip uninstall --yes accelerate torch\n!pip install \"unsloth==2024.9.post4\" \"torch==2.4.1\"\n#!pip install trl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_train.py\nimport os\nfrom model_runner import prepare_model, training_run\nimport time\nfrom arc_loader import *\n\n\nbase_path, running_on_kaggle = ('/kaggle', True) if os.path.exists('/kaggle') else ('.', False)\narc_challenge_file = os.path.join(base_path, 'input', 'arc-prize-2024', 'arc-agi_evaluation_challenges.json')\n#dataset = ArcDataset.from_file(arc_challenge_file)\ndataset = ArcDataset.from_file(\"/kaggle/working/sub_arc-agi_evaluation_challenges.json\")\n\n# Step 2: Prepare the model, tokenizer, and formatter\nmodel_id = \"chuanli11/Llama-3.2-3B-Instruct-uncensored\"  # base model, if needed explicitly\n#peft_adapter = \"jakebentley2001/arc-models\"  # your adapter repo containing adapter_config.json etc.\nmode = \"transformers_bf16_4bit\"\npeft_dict = {\n    \"r\": 8,        # rank\n    \"lora_alpha\": 16,   # scaling parameter\n    \"lora_dropout\": 0.1 # dropout probability\n    \n}\nmodel, tokenizer, _ = prepare_model(model_id, mode, peft=[peft_dict])\n\nformatter = ArcFormatter_premix_3(tokenizer=tokenizer)\n\ntmp_dir = os.path.join(base_path, 'temp')\n\n# Step 3: Set training parameters\ntrain_args = {\n    \"num_train_epochs\": 1,\n    \"per_device_train_batch_size\": 1,\n    \"learning_rate\": 5e-5,\n    \"output_dir\": os.path.join(tmp_dir, 'checkpoints'),\n    \"logging_dir\": os.path.join(tmp_dir, \"logs\"),  # Optionally write logs to a directory\n    \"log_level\": \"debug\", \n}\nmax_seq_length = 128\n\n# Step 4: Run the training loop on the single example\nmodel, trainer_stats = training_run(\n    model=model,\n    formatter=formatter,\n    dataset=dataset,\n    train_args=train_args,\n    max_seq_length=max_seq_length,\n    merge=False\n)\n\nprint(\"Training complete with stats:\", trainer_stats)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T02:29:37.321633Z","iopub.execute_input":"2025-04-11T02:29:37.322421Z","execution_failed":"2025-04-11T02:30:25.581Z"}},"outputs":[{"name":"stderr","text":"loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--chuanli11--Llama-3.2-3B-Instruct-uncensored/snapshots/27bd02b95b56f9886daf3d4be4101916b15809d1/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--chuanli11--Llama-3.2-3B-Instruct-uncensored/snapshots/27bd02b95b56f9886daf3d4be4101916b15809d1/special_tokens_map.json\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--chuanli11--Llama-3.2-3B-Instruct-uncensored/snapshots/27bd02b95b56f9886daf3d4be4101916b15809d1/tokenizer_config.json\n","output_type":"stream"},{"name":"stdout","text":"*** Load challanges from '/kaggle/working/sub_arc-agi_evaluation_challenges.json'...\n*** Load base model and tokenizer from 'chuanli11/Llama-3.2-3B-Instruct-uncensored'...\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--chuanli11--Llama-3.2-3B-Instruct-uncensored/snapshots/27bd02b95b56f9886daf3d4be4101916b15809d1/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"chuanli11/Llama-3.2-3B-Instruct-uncensored\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 24,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.44.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--chuanli11--Llama-3.2-3B-Instruct-uncensored/snapshots/27bd02b95b56f9886daf3d4be4101916b15809d1/model.safetensors.index.json\nInstantiating LlamaForCausalLM model under default dtype torch.bfloat16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ]\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5920e8dc613440ea93a7ec7a9ca5e03e"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint weights were used when initializing LlamaForCausalLM.\n\nAll the weights of LlamaForCausalLM were initialized from the model checkpoint at chuanli11/Llama-3.2-3B-Instruct-uncensored.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--chuanli11--Llama-3.2-3B-Instruct-uncensored/snapshots/27bd02b95b56f9886daf3d4be4101916b15809d1/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 128000,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"temperature\": 0.6,\n  \"top_p\": 0.9\n}\n\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"*** Create new peft model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c15fde7ac8274360a45527fe11306cce"}},"metadata":{}},{"name":"stderr","text":"Using auto half precision backend\nCurrently training with a batch size of: 2\n***** Running training *****\n  Num examples = 3\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Training with DataParallel so batch size has been adjusted to: 2\n  Total train batch size (w. parallel, distributed & accumulation) = 2\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2\n  Number of trainable parameters = 2,293,760\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"name":"stdout","text":"*** Start training run...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"from model_runner import *\nfrom arc_loader import *\n# model_id = \"jakebentley2001/arc-models\"\n# mode = \"transformers_bf16_4bit\"\n# model, tokenizer, _ = prepare_model(model_id, mode)\n\nmodel_id = \"chuanli11/Llama-3.2-3B-Instruct-uncensored\"  # base model, if needed explicitly\npeft_adapter = \"jakebentley2001/arc-models\"  # your adapter repo containing adapter_config.json etc.\nmode = \"transformers_bf16_4bit\"\nmodel, tokenizer, _ = prepare_model(model_id, mode, peft=[peft_adapter])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile common_stuff.py\n\n\nclass RemapCudaOOM:\n    def __enter__(self): pass\n    def __exit__(self, exc_type, exc_value, traceback):\n        oom_errors = [\"CUDA out of memory\", \"Make sure you have enough GPU RAM\", \"does not fit any GPU's remaining memory\"]\n        #Check if exc_value has any errors that are oom errors\n        if exc_value and any(x in str(exc_value) for x in oom_errors):\n            with open('submission.json', 'w') as f: f.write('cause submission scoring error')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from common_stuff import *\n\nwith RemapCudaOOM():\n    model = None\n    formatter = MyFormatter()\n    dataset = None\n    decoder = Decoder(formatter, arc_test_set.split_multi_replies(), n_guesses = 2, frac_score = True).from_store(infer_params[\"store\"])\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import json\n# # Path to your JSON file on Kaggle\n# file_path = \"/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json\"\n# # Open and load the JSON file\n# with open(file_path, 'r') as f:\n#     data = json.load(f)\n# # If your file has a \"root\" key, get its value; otherwise, use the loaded data as is.\n# root_data = data.get(\"root\", data)\n# # Print the keys of the dataset (e.g., \"00576224\", \"009d5c81\", etc.)\n# print(\"Keys in the dataset:\", list(root_data.keys()))\n# # Optionally, iterate through each item and print details\n# for key, item in root_data.items():\n#     train_examples = item.get(\"train\", [])\n#     test_examples = item.get(\"test\", [])\n#     print(f\"Key: {key}\")\n#     print(f\"  Number of train examples: {len(train_examples)}\")\n#     print(f\"  Number of test examples: {len(test_examples)}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}